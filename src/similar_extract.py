#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Similar Extract Pipeline
å®Œæ•´æµç¨‹ï¼šä¸Šä¼ PDF + æé—®query + topkå‚æ•°ï¼Œä¸²è”æ‰€æœ‰æ¨¡å—è¿›è¡Œç›¸ä¼¼åº¦æ£€ç´¢ä¸å…¬å¼æå–

æµç¨‹ï¼š
1. PDF â†’ MDï¼ˆpdf_to_mdï¼‰
2. MD â†’ chunksï¼ˆllm_retrieveï¼‰
3. chunks â†’ embeddingsï¼ˆembeddingï¼‰
4. query â†’ embeddingï¼ˆembeddingï¼‰
5. ç›¸ä¼¼åº¦è®¡ç®—å–topKï¼ˆquery_rerankï¼‰
6. ä»…å¯¹topK chunksåšå…¬å¼æ¸…æ´—/ä¿®å¤ä¸æå–ï¼ˆdevide_and_fix + llm_4_extractï¼‰
7. è¾“å‡ºï¼štopKç»“æœ + å…¬å¼æå–æ–‡ä»¶ + å‚æ•°æå–æ–‡ä»¶

ä¾èµ–ï¼š
- src/llm_retrieve.py (process_markdown_to_chunks)
- src/embedding.py (process_jsonl_to_embeddings)
- src/query_rerank/query_rerank.py (ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°)
- devide_and_fixã€llm_4_extractï¼ˆé€šè¿‡è°ƒç”¨å…¶ä¸»é€»è¾‘ï¼‰
"""

import json
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import sys
import os
import shutil

from llm_client import LLMConfig

# å¯¼å…¥å†…éƒ¨æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

# ==================== é…ç½® ====================
class Config:
    # åŸºç¡€è·¯å¾„ï¼ˆä»“åº“æ ¹ï¼‰
    ROOT = Path(__file__).resolve().parents[1]
    RUNTIME_DIR = ROOT / "data" / "runtime"
    FORMULAS_DIR = ROOT / "data" / "formulas" / "thesis"
    QUANTITIES_DIR = ROOT / "data" / "quantities" / "thesis"
    
    # MinerU API
    MINERU_API_BASE = "http://www.science42.vip:40093"
    
    # LLM é…ç½®
    LLM_API_KEY = "ximu-llm-api-key"
    LLM_API_BASE = "http://www.science42.vip:40200/v1/chat/completions"
    
    # SSH é…ç½®ï¼ˆç”¨äº embeddingï¼‰
    SSH_PASS = os.getenv("SSH_PASS", "")


# ç»Ÿä¸€çš„ LLM é…ç½®å®ä¾‹ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é»˜è®¤ï¼‰
LLM_CFG = LLMConfig(api_key=Config.LLM_API_KEY, base_url=Config.LLM_API_BASE)


def create_session_id() -> str:
    """ç”Ÿæˆä¼šè¯IDï¼šsession_YYYYMMDD_HHMMSS_6ä½éšæœº"""
    import random
    import string
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    rand = "".join(random.choices(string.ascii_uppercase + string.digits, k=6))
    return f"session_{ts}_{rand}"


def create_session_dir(session_id: str) -> Path:
    """åˆ›å»ºä¼šè¯ç›®å½•"""
    Config.RUNTIME_DIR.mkdir(parents=True, exist_ok=True)
    session_dir = Config.RUNTIME_DIR / session_id
    session_dir.mkdir(parents=True, exist_ok=True)
    return session_dir


async def extract_formulas_and_quantities_from_topk(
    topk_chunks: List[Dict[str, Any]],
    session_dir: Path,
    source_stem: str,
) -> Tuple[str, str]:
    """
    ä»…å¯¹ topK chunks åšå…¬å¼æ¸…æ´—/ä¿®å¤ä¸å‚æ•°æå–ã€‚
    
    å‚è€ƒ devide_and_fix.py å’Œ llm_4_extract.py çš„æ€è·¯ï¼š
    - æŠŠ topK chunks çš„ content è½¬æˆ LaTeX JSONL
    - è°ƒç”¨ devide_and_fix æ¸…æ´—/ä¿®å¤å…¬å¼
    - è°ƒç”¨ llm_4_extract æå–å‚æ•°
    
    è¾“å‡ºï¼š
    - formulas_file: å…¬å¼æå–æ–‡ä»¶è·¯å¾„
    - quantities_file: å‚æ•°æå–æ–‡ä»¶è·¯å¾„
    """
    try:
        # åŠ¨æ€å¯¼å…¥ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
        from offline_extract import process_local_pdf
        from devide_and_fix import process_and_fix_formulas
        from llm_4_extract import extract_formulas_to_yaml
        
        # æ­¥éª¤ 1: æ„å»º LaTeX JSONLï¼ˆæ¨¡ä»¿ md_to_latex çš„è¾“å‡ºæ ¼å¼ï¼‰
        latex_jsonl_path = session_dir / "topk_latex.jsonl"
        with latex_jsonl_path.open("w", encoding="utf-8") as f:
            for idx, chunk in enumerate(topk_chunks):
                content = chunk.get("content", "")
                # ç®€åŒ–çš„ LaTeX è®°å½•ï¼ˆä¿ç•™åŸ contentï¼‰
                record = {
                    "chunk_id": chunk.get("chunk_id"),
                    "content": content,
                    "formulas": [],  # å ä½ç¬¦
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        
        # æ­¥éª¤ 2: å…¬å¼æ¸…æ´—/ä¿®å¤ï¼ˆä½¿ç”¨ devide_and_fix é€»è¾‘ï¼‰
        fixed_latex_path = session_dir / "topk_latex_fixed.jsonl"
        try:
            fixed_latex_path = await process_and_fix_formulas(
                input_jsonl=str(latex_jsonl_path),
                output_dir=str(session_dir),
                md_file=None,
                api_key=Config.LLM_API_KEY,
                base_url=Config.LLM_API_BASE,
                llm_cfg=LLM_CFG,
            )
        except Exception as e:
            print(f"[WARNING] å…¬å¼ä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶: {e}")
            fixed_latex_path = latex_jsonl_path
        
        # æ­¥éª¤ 3: å‚æ•°æå–ï¼ˆä½¿ç”¨ llm_4_extract é€»è¾‘ï¼‰
        yaml_output_dir = await extract_formulas_to_yaml(
            input_jsonl=str(fixed_latex_path),
            output_dir=str(session_dir),
            api_key=Config.LLM_API_KEY,
            base_url=Config.LLM_API_BASE,
            llm_cfg=LLM_CFG,
        )

        # å°†è¾“å‡ºç§»åŠ¨åˆ°å›ºå®šç›®å½•ï¼Œå¹¶æŒ‰â€œåŸå§‹æ–‡ä»¶å_*.yamlâ€å‘½å
        src_dir = Path(yaml_output_dir)
        src_formulas = src_dir / "formulas.yaml"
        src_quantities = src_dir / "quantities.yaml"

        Config.FORMULAS_DIR.mkdir(parents=True, exist_ok=True)
        Config.QUANTITIES_DIR.mkdir(parents=True, exist_ok=True)

        dst_formulas = Config.FORMULAS_DIR / f"{source_stem}_formulas.yaml"
        dst_quantities = Config.QUANTITIES_DIR / f"{source_stem}_quantities.yaml"

        if not src_formulas.exists() or not src_quantities.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æœŸæœ›çš„ YAML: {src_formulas} / {src_quantities}")

        shutil.move(str(src_formulas), str(dst_formulas))
        shutil.move(str(src_quantities), str(dst_quantities))

        return str(dst_formulas), str(dst_quantities)
    
    except Exception as e:
        print(f"[ERROR] å…¬å¼ä¸å‚æ•°æå–å¤±è´¥: {e}")
        # è¿”å›ç©ºæ–‡ä»¶åæˆ–å ä½ç¬¦
        return "", ""


async def process_pipeline(
    pdf_path: str,
    query: str,
    topk: int = 10,
    session_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    å®Œæ•´æµç¨‹ï¼šPDF â†’ chunks â†’ embeddings â†’ ç›¸ä¼¼åº¦ â†’ topK â†’ å…¬å¼æå–
    
    å‚æ•°ï¼š
        pdf_path: ä¸Šä¼ çš„ PDF æ–‡ä»¶è·¯å¾„
        query: ç”¨æˆ·æé—®
        topk: è¿”å›çš„ topK æ•°é‡
        session_id: ä¼šè¯IDï¼ˆè‹¥ä¸ºç©ºåˆ™ç”Ÿæˆï¼‰
    
    è¿”å›ï¼š
        {
            "session_id": str,
            "topk": int,
            "results": [{score, chunk_id, content, ...}],
            "outputs": {formulas_file, quantities_file},
            "artifacts": {md_path, chunks_path, embeddings_path}
        }
    """
    if not session_id:
        session_id = create_session_id()
    
    session_dir = create_session_dir(session_id)
    
    print(f"\n{'='*70}")
    print(f"[ä¼šè¯] {session_id}")
    print(f"[å·¥ä½œç›®å½•] {session_dir}")
    print(f"{'='*70}\n")
    
    try:
        source_stem = Path(pdf_path).stem

        # ========== Step 1: PDF â†’ MD ==========
        print("[Step 1/6] PDF â†’ MD...")
        from pdf_to_md import extract_pdf_to_md_async
        md_path = await extract_pdf_to_md_async(
            input_path=pdf_path,
            output_dir=str(session_dir),
            docker_url=Config.MINERU_API_BASE,
        )
        if not md_path:
            return {"success": False, "error": "PDF è½¬ MD å¤±è´¥"}
        print(f"âœ… MD: {md_path}")
        
        # ========== Step 2: MD â†’ chunks ==========
        print("\n[Step 2/6] MD â†’ chunks...")
        from llm_retrieve import process_markdown_to_chunks
        chunks_path = process_markdown_to_chunks(
            input_md=md_path,
            out_dir=session_dir,
            verbose=True
        )
        print(f"âœ… chunks: {chunks_path}")
        
        # ========== Step 3: chunks â†’ embeddings ==========
        print("\n[Step 3/6] chunks â†’ embeddings...")
        from embedding import process_jsonl_to_embeddings
        embedded_path = process_jsonl_to_embeddings(
            input_jsonl=chunks_path,
            out_dir=session_dir,
            batch_size=32,
            verbose=True
        )
        print(f"âœ… embeddings: {embedded_path}")
        
        # ========== Step 4: query embedding + ç›¸ä¼¼åº¦è®¡ç®— ==========
        print("\n[Step 4/6] ç›¸ä¼¼åº¦è®¡ç®—...")
        from query_rerank import read_embedded_jsonl, fetch_embeddings_over_ssh, _cosine, SSHConfig
        
        # è¯»å–å·²åµŒå…¥çš„ chunks
        records = read_embedded_jsonl(Path(embedded_path))
        
        # query å‘é‡åŒ–ï¼ˆä¸ embedding.py ç›¸åŒçš„æœåŠ¡ï¼‰
        from embedding import escape_backslashes
        cfg = SSHConfig()
        texts = [escape_backslashes(query)]
        q_emb = fetch_embeddings_over_ssh(texts, batch_size=32, cfg=cfg)[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
        scored = []
        for rec in records:
            emb = rec.get("embedding")
            if isinstance(emb, list):
                score = _cosine(q_emb, emb)
                scored.append((score, rec))
        scored.sort(key=lambda x: x[0], reverse=True)
        
        # å– topK
        topk = max(1, min(topk, len(scored)))
        topk_results = [(score, rec) for score, rec in scored[:topk]]
        print(f"âœ… æ‰¾åˆ° {len(scored)} æ¡è®°å½•ï¼Œè¿”å› topK={topk}")
        
        # ========== Step 5: ä»…å¯¹ topK åšå…¬å¼æå– ==========
        print("\n[Step 5/6] å¯¹ topK chunks åšå…¬å¼/å‚æ•°æå–...")
        topk_chunks = [rec for _, rec in topk_results]
        formulas_file, quantities_file = await extract_formulas_and_quantities_from_topk(
            topk_chunks,
            session_dir,
            source_stem,
        )
        print(f"âœ… å…¬å¼æ–‡ä»¶: {formulas_file}")
        print(f"âœ… å‚æ•°æ–‡ä»¶: {quantities_file}")
        
        # ========== Step 6: ç»„ç»‡è¿”å›ç»“æœ ==========
        print("\n[Step 6/6] ç»„ç»‡è¿”å›ç»“æœ...")
        response = {
            "success": True,
            "session_id": session_id,
            "topk": topk,
            "results": [
                {
                    "score": float(score),
                    "chunk_id": rec.get("chunk_id"),
                    "content": rec.get("content"),
                    "page_hint": rec.get("page_hint"),
                    "section_path": rec.get("section_path"),
                }
                for score, rec in topk_results
            ],
            "outputs": {
                "formulas_file": formulas_file,
                "quantities_file": quantities_file,
            },
            "artifacts": {
                "md_path": str(md_path),
                "chunks_path": str(chunks_path),
                "embeddings_path": str(embedded_path),
                "session_dir": str(session_dir),
            }
        }
        
        print("\n" + "="*70)
        print("âœ… å¤„ç†å®Œæˆ")
        print("="*70 + "\n")
        
        return response
    
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e), "session_id": session_id}


# ==================== å‘½ä»¤è¡Œå…¥å£ ==========
async def main_cli():
    """å‘½ä»¤è¡Œæµ‹è¯•å…¥å£"""
    pdf = str((Config.ROOT / "data" / "raw" / "tilt_rotor" / "tilt_rotor.pdf").resolve())
    query = "æ—‹ç¿¼éœ€ç”¨åŠŸç‡å¦‚ä½•è®¡ç®—ï¼Ÿ"
    topk = 3
    
    result = await process_pipeline(pdf, query, topk)
    
    if result["success"]:
        print("\nğŸ“‹ è¿”å›ç»“æœæ‘˜è¦ï¼š")
        print(f"  session_id: {result['session_id']}")
        print(f"  topk: {result['topk']}")
        print(f"  ç»“æœæ¡æ•°: {len(result['results'])}")
        print(f"  å…¬å¼æ–‡ä»¶: {result['outputs']['formulas_file']}")
        print(f"  å‚æ•°æ–‡ä»¶: {result['outputs']['quantities_file']}")
        
        # æ‰“å° topK ç»“æœ
        print("\nğŸ“Œ TopK æœç´¢ç»“æœï¼š")
        for i, item in enumerate(result['results'], 1):
            print(f"\n  [{i}] score={item['score']:.4f} chunk_id={item['chunk_id']}")
            print(f"      content: {item['content'][:100]}...")
    else:
        print(f"\nâŒ å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    asyncio.run(main_cli())
